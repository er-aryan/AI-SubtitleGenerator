{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Subtitle Generation Notebook\n",
        "This notebook provides a step-by-step guide to generating subtitles for videos using various tools and libraries. Follow the instructions below to create subtitles for your video content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import subprocess\n",
        "import uuid\n",
        "import wave\n",
        "import contextlib\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    torch = None\n",
        "\n",
        "try:\n",
        "    import whisper\n",
        "except ImportError:\n",
        "    whisper = None\n",
        "\n",
        "try:\n",
        "    from transformers import pipeline as hf_asr_pipeline\n",
        "except ImportError:\n",
        "    hf_asr_pipeline = None\n",
        "\n",
        "try:\n",
        "    from transformers import MarianMTModel, MarianTokenizer\n",
        "except ImportError:\n",
        "    MarianMTModel = None\n",
        "    MarianTokenizer = None\n",
        "\n",
        "try:\n",
        "    import nemo.collections.asr as nemo_asr\n",
        "except ImportError:\n",
        "    nemo_asr = None\n",
        "\n",
        "LANG_CODE_MAP = {\n",
        "    'hindi': 'hi', 'marathi': 'mr', 'spanish': 'es', 'french': 'fr', 'german': 'de',\n",
        "    'japanese': 'ja', 'chinese': 'zh', 'arabic': 'ar', 'english': 'en', 'tamil': 'ta',\n",
        "    'telugu': 'te', 'bengali': 'bn', 'kannada': 'kn', 'gujarati': 'gu', 'punjabi': 'pa'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensure_dir(path: Path) -> Path:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "\n",
        "def run_ffmpeg(command: List[str]) -> None:\n",
        "    completed = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)\n",
        "    if completed.returncode != 0:\n",
        "        raise RuntimeError(completed.stderr.decode('utf-8', errors='ignore'))\n",
        "\n",
        "\n",
        "def extract_audio_ffmpeg(video_path: Path, output_dir: Path, sample_rate: int = 16000) -> Path:\n",
        "    ensure_dir(output_dir)\n",
        "    audio_path = output_dir / f'{video_path.stem}_{uuid.uuid4().hex[:8]}.wav'\n",
        "    command = [\n",
        "        'ffmpeg', '-y', '-i', str(video_path),\n",
        "        '-ac', '1', '-ar', str(sample_rate), str(audio_path)\n",
        "    ]\n",
        "    run_ffmpeg(command)\n",
        "    return audio_path\n",
        "\n",
        "\n",
        "def get_audio_duration(audio_path: Path) -> float:\n",
        "    with contextlib.closing(wave.open(str(audio_path), 'rb')) as wf:\n",
        "        frames = wf.getnframes()\n",
        "        rate = wf.getframerate()\n",
        "    return frames / float(rate)\n",
        "\n",
        "\n",
        "def format_timestamp(seconds: float) -> str:\n",
        "    seconds = max(seconds, 0.0)\n",
        "    h, rem = divmod(int(seconds), 3600)\n",
        "    m, s = divmod(rem, 60)\n",
        "    ms = int(round((seconds - int(seconds)) * 1000))\n",
        "    return f'{h:02}:{m:02}:{s:02},{ms:03}'\n",
        "\n",
        "\n",
        "def segments_to_srt(segments: List[Dict[str, Any]], output_path: Path) -> None:\n",
        "    ensure_dir(output_path.parent)\n",
        "    with open(output_path, 'w', encoding='utf-8') as handle:\n",
        "        for idx, segment in enumerate(segments, start=1):\n",
        "            start_ts = format_timestamp(segment.get('start', 0.0))\n",
        "            end_ts = format_timestamp(segment.get('end', segment.get('start', 0.0)))\n",
        "            text = segment.get('text', '').strip()\n",
        "            handle.write(f'{idx}\\n{start_ts} --> {end_ts}\\n{text}\\n\\n')\n",
        "\n",
        "\n",
        "def aggregate_words(words: List[Dict[str, Any]], max_words: int = 10) -> List[Dict[str, Any]]:\n",
        "    segments, buffer, start_time = [], [], None\n",
        "    for word in words:\n",
        "        token = word.get('word', '').strip()\n",
        "        if not token:\n",
        "            continue\n",
        "        if start_time is None:\n",
        "            start_time = word.get('start', 0.0)\n",
        "        buffer.append(token)\n",
        "        end_time = word.get('end', start_time)\n",
        "        if len(buffer) >= max_words:\n",
        "            segments.append({'start': start_time, 'end': end_time, 'text': ' '.join(buffer)})\n",
        "            buffer, start_time = [], None\n",
        "    if buffer:\n",
        "        end_time = words[-1].get('end', start_time or 0.0)\n",
        "        segments.append({'start': start_time or 0.0, 'end': end_time, 'text': ' '.join(buffer)})\n",
        "    return segments\n",
        "\n",
        "\n",
        "def approximate_segments_from_text(text: str, audio_duration: float, words_per_segment: int = 16) -> List[Dict[str, Any]]:\n",
        "    tokens = text.strip().split()\n",
        "    if not tokens or audio_duration <= 0:\n",
        "        return []\n",
        "    avg_time = audio_duration / max(len(tokens), 1)\n",
        "    segments = []\n",
        "    start = 0.0\n",
        "    idx = 0\n",
        "    while idx < len(tokens):\n",
        "        chunk = tokens[idx: idx + words_per_segment]\n",
        "        chunk_duration = avg_time * len(chunk)\n",
        "        end = min(start + chunk_duration, audio_duration)\n",
        "        segments.append({'start': start, 'end': end, 'text': ' '.join(chunk)})\n",
        "        start = end\n",
        "        idx += words_per_segment\n",
        "    if segments:\n",
        "        segments[-1]['end'] = audio_duration\n",
        "    return segments\n",
        "\n",
        "\n",
        "def preview_segments(name: str, segments: List[Dict[str, Any]], limit: int = 3) -> None:\n",
        "    print('\\n{} -> {} segments'.format(name, len(segments)))\n",
        "    for segment in segments[:limit]:\n",
        "        print('  [{} - {}] {}'.format(format_timestamp(segment['start']), format_timestamp(segment['end']), segment['text']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Input Paths\n",
        "Update `VIDEO_PATH` to point to the video you want to process. The cell extracts mono 16 kHz audio so every ASR model can share the same waveform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Audio extracted to output/your_video/audio/your_video_c11c97cc.wav\n",
            "Duration: 80.13 seconds\n"
          ]
        }
      ],
      "source": [
        "VIDEO_PATH = Path('media/your_video.mp4')\n",
        "assert VIDEO_PATH.exists(), f'Video not found: {VIDEO_PATH}'\n",
        "\n",
        "BASE_NAME = VIDEO_PATH.stem\n",
        "NOTEBOOK_OUTPUT = ensure_dir(Path('output') / BASE_NAME)\n",
        "AUDIO_DIR = ensure_dir(NOTEBOOK_OUTPUT / 'audio')\n",
        "SRT_DIR = ensure_dir(NOTEBOOK_OUTPUT / 'srt')\n",
        "TRANSLATION_DIR = ensure_dir(NOTEBOOK_OUTPUT / 'translations')\n",
        "\n",
        "AUDIO_PATH = extract_audio_ffmpeg(VIDEO_PATH, AUDIO_DIR)\n",
        "AUDIO_DURATION = get_audio_duration(AUDIO_PATH)\n",
        "print(f'Audio extracted to {AUDIO_PATH}')\n",
        "print(f'Duration: {AUDIO_DURATION:.2f} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Segment 1 · Transcription\n",
        "Each subsection runs a different speech recogniser. Results are cached inside `transcripts_by_model` and written to disk as English `.srt` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "transcripts_by_model: Dict[str, List[Dict[str, Any]]] = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Whisper -> 20 segments\n",
            "  [00:00:00,000 - 00:00:05,320] Hello and welcome to One Minute Wednesdays where I teach you English in just one minute.\n",
            "  [00:00:05,320 - 00:00:11,280] You solidify your knowledge by writing a practice sentence down below and I will give it a thumb up\n",
            "  [00:00:11,280 - 00:00:14,600] if it's correct and I will try to correct you if it's not.\n",
            "Saved -> output/your_video/srt/your_video_whisper.srt\n"
          ]
        }
      ],
      "source": [
        "model_size = 'small'  # change to tiny/base/medium/large as needed\n",
        "try:\n",
        "    if whisper is None:\n",
        "        raise ImportError('whisper package not installed')\n",
        "    whisper_model = whisper.load_model(model_size)\n",
        "    result = whisper_model.transcribe(str(AUDIO_PATH))\n",
        "    whisper_segments = [\n",
        "        {'start': seg['start'], 'end': seg['end'], 'text': seg['text'].strip()}\n",
        "        for seg in result.get('segments', [])\n",
        "    ]\n",
        "    transcripts_by_model['whisper'] = whisper_segments\n",
        "    whisper_srt = SRT_DIR / f'{BASE_NAME}_whisper.srt'\n",
        "    segments_to_srt(whisper_segments, whisper_srt)\n",
        "    preview_segments('Whisper', whisper_segments)\n",
        "    print(f'Saved -> {whisper_srt}')\n",
        "except Exception as exc:\n",
        "    print(f'Whisper transcription skipped: {exc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wav2Vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use mps:0\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Wav2Vec2 -> 175 segments\n",
            "  [00:00:00,320 - 00:00:00,620] HELLO\n",
            "  [00:00:00,780 - 00:00:00,880] AND\n",
            "  [00:00:00,920 - 00:00:01,220] WELCOME\n",
            "Saved -> output/your_video/srt/your_video_wav2vec2.srt\n"
          ]
        }
      ],
      "source": [
        "wav2vec_model_id = 'facebook/wav2vec2-large-960h-lv60-self'\n",
        "try:\n",
        "    if hf_asr_pipeline is None:\n",
        "        raise ImportError('transformers pipeline not available')\n",
        "    wav2vec_pipeline = hf_asr_pipeline(\n",
        "        task='automatic-speech-recognition',\n",
        "        model=wav2vec_model_id,\n",
        "        chunk_length_s=30,\n",
        "        stride_length_s=5,\n",
        "        return_timestamps='word'\n",
        "    )\n",
        "    wav2vec_result = wav2vec_pipeline(str(AUDIO_PATH))\n",
        "    if 'chunks' in wav2vec_result:\n",
        "        wav2vec_segments = [\n",
        "            {'start': float(chunk['timestamp'][0]), 'end': float(chunk['timestamp'][1]), 'text': chunk['text'].strip()}\n",
        "            for chunk in wav2vec_result['chunks']\n",
        "            if chunk.get('timestamp') and chunk.get('text', '').strip()\n",
        "        ]\n",
        "    else:\n",
        "        wav2vec_segments = approximate_segments_from_text(wav2vec_result.get('text', ''), AUDIO_DURATION)\n",
        "    transcripts_by_model['wav2vec2'] = wav2vec_segments\n",
        "    wav2vec_srt = SRT_DIR / f'{BASE_NAME}_wav2vec2.srt'\n",
        "    segments_to_srt(wav2vec_segments, wav2vec_srt)\n",
        "    preview_segments('Wav2Vec2', wav2vec_segments)\n",
        "    print(f'Saved -> {wav2vec_srt}')\n",
        "except Exception as exc:\n",
        "    print(f'Wav2Vec2 transcription skipped: {exc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Silero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /Users/aryansharma/.cache/torch/hub/snakers4_silero-models_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Silero -> 9 segments\n",
            "  [00:00:00,000 - 00:00:09,787] to edesday where i to english just one minute you little y by writing a act\n",
            "  [00:00:09,787 - 00:00:19,574] sentence down low and i will if it's correct i wouldtry to rectyou if it's not\n",
            "  [00:00:19,574 - 00:00:29,361] so let's start that clock i us the phoneones they are se differently but sound the\n",
            "Saved -> output/your_video/srt/your_video_silero.srt\n"
          ]
        }
      ],
      "source": [
        "silero_device = 'cuda' if torch and torch.cuda.is_available() else 'cpu'\n",
        "try:\n",
        "    if torch is None:\n",
        "        raise ImportError('torch not installed')\n",
        "\n",
        "    silero_model, silero_decoder, silero_utils = torch.hub.load(\n",
        "        repo_or_dir='snakers4/silero-models',\n",
        "        model='silero_stt',\n",
        "        language='en',\n",
        "        device=silero_device\n",
        "    )\n",
        "\n",
        "    read_batch, split_into_batches, read_audio, prepare_model_input = silero_utils\n",
        "\n",
        "    silero_batches = split_into_batches([str(AUDIO_PATH)], batch_size=1)\n",
        "    silero_text = []\n",
        "\n",
        "    for batch in silero_batches:\n",
        "        audio = read_batch(batch)\n",
        "        input_tensor = prepare_model_input(audio).to(silero_device)\n",
        "        output = silero_model(input_tensor)\n",
        "        silero_text.append(silero_decoder(output[0].cpu()))\n",
        "\n",
        "    combined_text = ' '.join(silero_text)\n",
        "    silero_segments = approximate_segments_from_text(combined_text, AUDIO_DURATION)\n",
        "    transcripts_by_model['silero'] = silero_segments\n",
        "\n",
        "    silero_srt = SRT_DIR / f'{BASE_NAME}_silero.srt'\n",
        "    segments_to_srt(silero_segments, silero_srt)\n",
        "    preview_segments('Silero', silero_segments)\n",
        "\n",
        "    print(f'Saved -> {silero_srt}')\n",
        "\n",
        "except Exception as exc:\n",
        "    print(f'Silero transcription skipped: {exc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NeMo (Kaldi-inspired)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2025-09-26 15:17:55 nemo_logging:393] Found existing object /Users/aryansharma/.cache/torch/NeMo/NeMo_2.4.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n",
            "[NeMo I 2025-09-26 15:17:55 nemo_logging:393] Re-using file from: /Users/aryansharma/.cache/torch/NeMo/NeMo_2.4.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo\n",
            "[NeMo I 2025-09-26 15:17:55 nemo_logging:393] Re-using file from: /Users/aryansharma/.cache/torch/NeMo/NeMo_2.4.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo\n",
            "[NeMo I 2025-09-26 15:17:55 nemo_logging:393] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2025-09-26 15:17:55 nemo_logging:393] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2025-09-26 15:17:55 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
            "[NeMo I 2025-09-26 15:17:55 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2025-09-26 15:17:55 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 64\n",
            "    shuffle: true\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    trim_silence: false\n",
            "    max_duration: 20.0\n",
            "    min_duration: 0.1\n",
            "    shuffle_n: 2048\n",
            "    is_tarred: true\n",
            "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
            "    \n",
            "[NeMo W 2025-09-26 15:17:55 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath:\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 64\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: na\n",
            "    \n",
            "[NeMo W 2025-09-26 15:17:55 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath:\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 64\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: na\n",
            "    \n",
            "[NeMo W 2025-09-26 15:17:55 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath:\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 64\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: na\n",
            "    \n",
            "[NeMo W 2025-09-26 15:17:55 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath:\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 64\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: na\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2025-09-26 15:17:55 nemo_logging:393] PADDING: 0\n",
            "[NeMo I 2025-09-26 15:17:55 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from /Users/aryansharma/.cache/torch/NeMo/NeMo_2.4.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n",
            "[NeMo I 2025-09-26 15:17:55 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from /Users/aryansharma/.cache/torch/NeMo/NeMo_2.4.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Transcribing: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "NeMo -> 11 segments\n",
            "  [00:00:00,000 - 00:00:07,454] hello i welcome two one minute wednesdays where i teach you english in just one minute\n",
            "  [00:00:07,454 - 00:00:14,908] you solidify your knowledge by writing a practice sentence down below and i will give it\n",
            "  [00:00:14,908 - 00:00:22,362] a thumb up if it's correct and i will try to correct you if it's not\n",
            "Saved -> output/your_video/srt/your_video_nemo.srt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "nemo_model_name = 'stt_en_conformer_ctc_small'\n",
        "try:\n",
        "    if nemo_asr is None:\n",
        "        raise ImportError('nemo-toolkit not installed')\n",
        "    nemo_model = nemo_asr.models.ASRModel.from_pretrained(model_name=nemo_model_name)\n",
        "    try:\n",
        "        transcribe_result = nemo_model.transcribe([str(AUDIO_PATH)], return_timestamps='word')\n",
        "    except TypeError:\n",
        "        transcribe_result = nemo_model.transcribe([str(AUDIO_PATH)])\n",
        "    nemo_transcripts = []\n",
        "    nemo_word_ts = None\n",
        "\n",
        "    if isinstance(transcribe_result, (list, tuple)) and len(transcribe_result) == 2:\n",
        "        possible_transcripts, possible_word_ts = transcribe_result[0], transcribe_result[1]\n",
        "        nemo_transcripts = possible_transcripts\n",
        "        nemo_word_ts = possible_word_ts\n",
        "    else:\n",
        "        nemo_transcripts = transcribe_result\n",
        "    def _coerce_text(obj):\n",
        "        if obj is None:\n",
        "            return ''\n",
        "        if isinstance(obj, str):\n",
        "            return obj\n",
        "        if hasattr(obj, 'text'):\n",
        "            try:\n",
        "                return obj.text\n",
        "            except Exception:\n",
        "                pass\n",
        "        try:\n",
        "            return str(obj)\n",
        "        except Exception:\n",
        "            return ''\n",
        "\n",
        "    if isinstance(nemo_transcripts, (list, tuple)):\n",
        "        nemo_transcripts = [_coerce_text(t) for t in nemo_transcripts]\n",
        "    elif isinstance(nemo_transcripts, str):\n",
        "        nemo_transcripts = [nemo_transcripts]\n",
        "    else:\n",
        "        try:\n",
        "            nemo_transcripts = [_coerce_text(t) for t in list(nemo_transcripts)]\n",
        "        except Exception:\n",
        "            nemo_transcripts = []\n",
        "    if nemo_word_ts:\n",
        "        normalized_word_ts = []\n",
        "        for utt in nemo_word_ts:\n",
        "            utt_tokens = []\n",
        "            for tok in utt:\n",
        "                if isinstance(tok, dict):\n",
        "                    word = tok.get('word') or tok.get('text') or ''\n",
        "                    start = tok.get('start_time', tok.get('start', 0.0))\n",
        "                    end = tok.get('end_time', tok.get('end', start))\n",
        "                else:\n",
        "                    # object with attributes\n",
        "                    word = getattr(tok, 'word', None) or getattr(tok, 'text', None) or str(tok)\n",
        "                    start = getattr(tok, 'start_time', None)\n",
        "                    if start is None:\n",
        "                        start = getattr(tok, 'start', 0.0)\n",
        "                    end = getattr(tok, 'end_time', None)\n",
        "                    if end is None:\n",
        "                        end = getattr(tok, 'end', start)\n",
        "                try:\n",
        "                    start = float(start) if start is not None else 0.0\n",
        "                except Exception:\n",
        "                    start = 0.0\n",
        "                try:\n",
        "                    end = float(end) if end is not None else start\n",
        "                except Exception:\n",
        "                    end = start\n",
        "                utt_tokens.append({'word': str(word).strip(), 'start': start, 'end': end})\n",
        "            normalized_word_ts.append(utt_tokens)\n",
        "        nemo_word_ts = normalized_word_ts\n",
        "\n",
        "    words = []\n",
        "    if nemo_word_ts and len(nemo_word_ts) > 0:\n",
        "        # use the first utterance's tokens (we only provided one file)\n",
        "        for token in nemo_word_ts[0]:\n",
        "            words.append({'word': token.get('word', ''), 'start': token.get('start', 0.0), 'end': token.get('end', 0.0)})\n",
        "        nemo_segments = aggregate_words(words)\n",
        "    elif nemo_transcripts:\n",
        "        # no timestamps — create approximate segments from transcript text\n",
        "        nemo_segments = approximate_segments_from_text(nemo_transcripts[0], AUDIO_DURATION)\n",
        "    else:\n",
        "        nemo_segments = []\n",
        "\n",
        "    transcripts_by_model['nemo'] = nemo_segments\n",
        "    nemo_srt = SRT_DIR / f'{BASE_NAME}_nemo.srt'\n",
        "    segments_to_srt(nemo_segments, nemo_srt)\n",
        "    preview_segments('NeMo', nemo_segments)\n",
        "    print(f'Saved -> {nemo_srt}')\n",
        "except Exception as exc:\n",
        "    print(f'NeMo transcription skipped: {type(exc).__name__}: {exc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vosk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
            "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
            "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from models/vosk-model-small-en-us-0.15/ivector/final.ie\n",
            "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
            "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from models/vosk-model-small-en-us-0.15/graph/HCLr.fst models/vosk-model-small-en-us-0.15/graph/Gr.fst\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo models/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Vosk -> 18 segments\n",
            "  [00:00:00,270 - 00:00:03,180] hello and welcome to woman it wednesday's where i teach\n",
            "  [00:00:03,270 - 00:00:07,290] english in just one minute you solidify our knowledge by\n",
            "  [00:00:07,290 - 00:00:10,350] writing a practice sentence down below and i will give\n",
            "Saved -> output/your_video/srt/your_video_vosk.srt\n"
          ]
        }
      ],
      "source": [
        "vosk_model_dir = Path('models/vosk-model-small-en-us-0.15')\n",
        "try:\n",
        "    from vosk import Model, KaldiRecognizer\n",
        "    if not vosk_model_dir.exists():\n",
        "        raise FileNotFoundError(f'Download a Vosk model to {vosk_model_dir}')\n",
        "    vosk_model = Model(str(vosk_model_dir))\n",
        "    wf = wave.open(str(AUDIO_PATH), 'rb')\n",
        "    recognizer = KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "    recognizer.SetWords(True)\n",
        "    words = []\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        if recognizer.AcceptWaveform(data):\n",
        "            partial = json.loads(recognizer.Result())\n",
        "            words.extend(partial.get('result', []))\n",
        "    final = json.loads(recognizer.FinalResult())\n",
        "    words.extend(final.get('result', []))\n",
        "    wf.close()\n",
        "    vosk_segments = aggregate_words(words)\n",
        "    transcripts_by_model['vosk'] = vosk_segments\n",
        "    vosk_srt = SRT_DIR / f'{BASE_NAME}_vosk.srt'\n",
        "    segments_to_srt(vosk_segments, vosk_srt)\n",
        "    preview_segments('Vosk', vosk_segments)\n",
        "    print(f'Saved -> {vosk_srt}')\n",
        "except Exception as exc:\n",
        "    print(f'Vosk transcription skipped: {exc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Segment 2 · Translation\n",
        "Provide the language names (or ISO codes) you want to generate. Translations reuse the English segments created above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating to -> ['hi', 'mr']\n"
          ]
        }
      ],
      "source": [
        "target_languages = ['hindi', 'marathi']  # supports names or ISO codes from LANG_CODE_MAP\n",
        "\n",
        "def resolve_language(lang: str) -> str:\n",
        "    lang = lang.lower().strip()\n",
        "    if lang in LANG_CODE_MAP.values():\n",
        "        return lang\n",
        "    return LANG_CODE_MAP.get(lang, 'en')\n",
        "\n",
        "resolved_targets = [resolve_language(lang) for lang in target_languages]\n",
        "print(f'Translating to -> {resolved_targets}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WHISPER -> HI -> 20 segments\n",
            "  [00:00:00,000 - 00:00:05,320] नमस्कार और एक मिनट के लिए स्वागत है जहां मैं सिर्फ एक मिनट में तुम्हें अंग्रेजी सिखाता हूं.\n",
            "  [00:00:05,320 - 00:00:11,280] आप नीचे दिये गये वाक्य को लिख कर अपने ज्ञान को मज़बूत करते हैं और मैं उसे इंच दूँगा\n",
            "  [00:00:11,280 - 00:00:14,600] अगर यह सही है और मैं आपको सही करने की कोशिश करेंगे अगर यह नहीं है.\n",
            "Saved -> output/your_video/translations/hi/your_video_whisper_hi.srt\n",
            "\n",
            "WHISPER -> MR -> 20 segments\n",
            "  [00:00:00,000 - 00:00:05,320] मी तुला इंग्रजीचा अभ्यास करायला शिकवू शकते.\n",
            "  [00:00:05,320 - 00:00:11,280] मी तुला ज्ञान व समज देईन. मी तुला ज्ञान व समज देईन.\n",
            "  [00:00:11,280 - 00:00:14,600] मी तुला तसं करायला देऊ शकत नाही.\n",
            "Saved -> output/your_video/translations/mr/your_video_whisper_mr.srt\n",
            "\n",
            "WHISPER -> MR -> 20 segments\n",
            "  [00:00:00,000 - 00:00:05,320] मी तुला इंग्रजीचा अभ्यास करायला शिकवू शकते.\n",
            "  [00:00:05,320 - 00:00:11,280] मी तुला ज्ञान व समज देईन. मी तुला ज्ञान व समज देईन.\n",
            "  [00:00:11,280 - 00:00:14,600] मी तुला तसं करायला देऊ शकत नाही.\n",
            "Saved -> output/your_video/translations/mr/your_video_whisper_mr.srt\n",
            "\n",
            "WAV2VEC2 -> HI -> 175 segments\n",
            "  [00:00:00,320 - 00:00:00,620] HELLO\n",
            "  [00:00:00,780 - 00:00:00,880] और\n",
            "  [00:00:00,920 - 00:00:01,220] कृपा\n",
            "Saved -> output/your_video/translations/hi/your_video_wav2vec2_hi.srt\n",
            "\n",
            "WAV2VEC2 -> HI -> 175 segments\n",
            "  [00:00:00,320 - 00:00:00,620] HELLO\n",
            "  [00:00:00,780 - 00:00:00,880] और\n",
            "  [00:00:00,920 - 00:00:01,220] कृपा\n",
            "Saved -> output/your_video/translations/hi/your_video_wav2vec2_hi.srt\n",
            "\n",
            "WAV2VEC2 -> MR -> 175 segments\n",
            "  [00:00:00,320 - 00:00:00,620] KCharselect unicode block name\n",
            "  [00:00:00,780 - 00:00:00,880] A button on a Remote Control\n",
            "  [00:00:00,920 - 00:00:01,220] सा. यु. पू.\n",
            "Saved -> output/your_video/translations/mr/your_video_wav2vec2_mr.srt\n",
            "\n",
            "WAV2VEC2 -> MR -> 175 segments\n",
            "  [00:00:00,320 - 00:00:00,620] KCharselect unicode block name\n",
            "  [00:00:00,780 - 00:00:00,880] A button on a Remote Control\n",
            "  [00:00:00,920 - 00:00:01,220] सा. यु. पू.\n",
            "Saved -> output/your_video/translations/mr/your_video_wav2vec2_mr.srt\n",
            "\n",
            "SILERO -> HI -> 9 segments\n",
            "  [00:00:00,000 - 00:00:09,787] दिन जहां मैं Nuck करने के लिए सिर्फ एक मिनट आप एक कार्य लिखने के द्वारा थोड़ा सा y\n",
            "  [00:00:09,787 - 00:00:19,574] नीचे वाक्य नीचे और मैं होगा अगर यह सही है आप सुधार करने के लिए अगर यह नहीं है\n",
            "  [00:00:19,574 - 00:00:29,361] तो चलो शुरू करते हैं कि घड़ी मैं हम फोनो, वे अलग अलग तरह से कर रहे हैं लेकिन ध्वनि\n",
            "Saved -> output/your_video/translations/hi/your_video_silero_hi.srt\n",
            "\n",
            "SILERO -> HI -> 9 segments\n",
            "  [00:00:00,000 - 00:00:09,787] दिन जहां मैं Nuck करने के लिए सिर्फ एक मिनट आप एक कार्य लिखने के द्वारा थोड़ा सा y\n",
            "  [00:00:09,787 - 00:00:19,574] नीचे वाक्य नीचे और मैं होगा अगर यह सही है आप सुधार करने के लिए अगर यह नहीं है\n",
            "  [00:00:19,574 - 00:00:29,361] तो चलो शुरू करते हैं कि घड़ी मैं हम फोनो, वे अलग अलग तरह से कर रहे हैं लेकिन ध्वनि\n",
            "Saved -> output/your_video/translations/hi/your_video_silero_hi.srt\n",
            "\n",
            "SILERO -> MR -> 9 segments\n",
            "  [00:00:00,000 - 00:00:09,787] [ चित्राचे श्रेय]\n",
            "  [00:00:09,787 - 00:00:19,574] अनुप्रयोगाची तपासणी करा व बाहेर पडा\n",
            "  [00:00:19,574 - 00:00:29,361] या प्रश् नांची उत्तरे आपण या लेखात पाहणार आहोत.\n",
            "Saved -> output/your_video/translations/mr/your_video_silero_mr.srt\n",
            "\n",
            "SILERO -> MR -> 9 segments\n",
            "  [00:00:00,000 - 00:00:09,787] [ चित्राचे श्रेय]\n",
            "  [00:00:09,787 - 00:00:19,574] अनुप्रयोगाची तपासणी करा व बाहेर पडा\n",
            "  [00:00:19,574 - 00:00:29,361] या प्रश् नांची उत्तरे आपण या लेखात पाहणार आहोत.\n",
            "Saved -> output/your_video/translations/mr/your_video_silero_mr.srt\n",
            "\n",
            "NEMO -> HI -> 11 segments\n",
            "  [00:00:00,000 - 00:00:07,454] हैलो मैं दो मिनट का स्वागत करता हूँ... ... दो मिनट हमन के दिनों जहां मैं तुम्हें सिर्फ एक मिनट में Friock सिखाता हूं\n",
            "  [00:00:07,454 - 00:00:14,908] नीचे टाइप किए वाक्य को लिख कर अपने ज्ञान को मज़बूत कीजिए और मैं इसे दे दूँगा\n",
            "  [00:00:14,908 - 00:00:22,362] अगर यह सही है और मैं आपको सुधार करने की कोशिश करेंगे अगर यह नहीं है\n",
            "Saved -> output/your_video/translations/hi/your_video_nemo_hi.srt\n",
            "\n",
            "NEMO -> HI -> 11 segments\n",
            "  [00:00:00,000 - 00:00:07,454] हैलो मैं दो मिनट का स्वागत करता हूँ... ... दो मिनट हमन के दिनों जहां मैं तुम्हें सिर्फ एक मिनट में Friock सिखाता हूं\n",
            "  [00:00:07,454 - 00:00:14,908] नीचे टाइप किए वाक्य को लिख कर अपने ज्ञान को मज़बूत कीजिए और मैं इसे दे दूँगा\n",
            "  [00:00:14,908 - 00:00:22,362] अगर यह सही है और मैं आपको सुधार करने की कोशिश करेंगे अगर यह नहीं है\n",
            "Saved -> output/your_video/translations/hi/your_video_nemo_hi.srt\n",
            "\n",
            "NEMO -> MR -> 11 segments\n",
            "  [00:00:00,000 - 00:00:07,454] [ चित्राचे श्रेय]\n",
            "  [00:00:07,454 - 00:00:14,908] ज्ञान आणि समज तुझ्या हृदयात ठेव. आणि तुझ्या हृदयात काय आहे ते समजून घे.\n",
            "  [00:00:14,908 - 00:00:22,362] योग्य निर्णय घेण्यासाठी तुम्ही कोणती पावले उचलू शकता?\n",
            "Saved -> output/your_video/translations/mr/your_video_nemo_mr.srt\n",
            "\n",
            "NEMO -> MR -> 11 segments\n",
            "  [00:00:00,000 - 00:00:07,454] [ चित्राचे श्रेय]\n",
            "  [00:00:07,454 - 00:00:14,908] ज्ञान आणि समज तुझ्या हृदयात ठेव. आणि तुझ्या हृदयात काय आहे ते समजून घे.\n",
            "  [00:00:14,908 - 00:00:22,362] योग्य निर्णय घेण्यासाठी तुम्ही कोणती पावले उचलू शकता?\n",
            "Saved -> output/your_video/translations/mr/your_video_nemo_mr.srt\n",
            "\n",
            "VOSK -> HI -> 18 segments\n",
            "  [00:00:00,270 - 00:00:03,180] हैलो और महिला के लिए स्वागत है यह Wdons दिवस जहां मैं सिखाता हूँ\n",
            "  [00:00:03,270 - 00:00:07,290] एक मिनट में केवल तुम हमारे ज्ञान को मज़बूत करते हो\n",
            "  [00:00:07,290 - 00:00:10,350] नीचे दिये गये वाक्य को लिख कर तथा मैं दे देंगे\n",
            "Saved -> output/your_video/translations/hi/your_video_vosk_hi.srt\n",
            "\n",
            "VOSK -> HI -> 18 segments\n",
            "  [00:00:00,270 - 00:00:03,180] हैलो और महिला के लिए स्वागत है यह Wdons दिवस जहां मैं सिखाता हूँ\n",
            "  [00:00:03,270 - 00:00:07,290] एक मिनट में केवल तुम हमारे ज्ञान को मज़बूत करते हो\n",
            "  [00:00:07,290 - 00:00:10,350] नीचे दिये गये वाक्य को लिख कर तथा मैं दे देंगे\n",
            "Saved -> output/your_video/translations/hi/your_video_vosk_hi.srt\n",
            "\n",
            "VOSK -> MR -> 18 segments\n",
            "  [00:00:00,270 - 00:00:03,180] आम्ही तिला सांगितलं, की तिला तिच्या बाळाचं नाव माहीत आहे.\n",
            "  [00:00:03,270 - 00:00:07,290] [ ३० पानांवरील चित्र]\n",
            "  [00:00:07,290 - 00:00:10,350] संगणकाची यादी बनवा व बाहेर पडा\n",
            "Saved -> output/your_video/translations/mr/your_video_vosk_mr.srt\n",
            "\n",
            "VOSK -> MR -> 18 segments\n",
            "  [00:00:00,270 - 00:00:03,180] आम्ही तिला सांगितलं, की तिला तिच्या बाळाचं नाव माहीत आहे.\n",
            "  [00:00:03,270 - 00:00:07,290] [ ३० पानांवरील चित्र]\n",
            "  [00:00:07,290 - 00:00:10,350] संगणकाची यादी बनवा व बाहेर पडा\n",
            "Saved -> output/your_video/translations/mr/your_video_vosk_mr.srt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[PosixPath('output/your_video/translations/hi/your_video_whisper_hi.srt'),\n",
              " PosixPath('output/your_video/translations/mr/your_video_whisper_mr.srt'),\n",
              " PosixPath('output/your_video/translations/hi/your_video_wav2vec2_hi.srt'),\n",
              " PosixPath('output/your_video/translations/mr/your_video_wav2vec2_mr.srt'),\n",
              " PosixPath('output/your_video/translations/hi/your_video_silero_hi.srt'),\n",
              " PosixPath('output/your_video/translations/mr/your_video_silero_mr.srt'),\n",
              " PosixPath('output/your_video/translations/hi/your_video_nemo_hi.srt'),\n",
              " PosixPath('output/your_video/translations/mr/your_video_nemo_mr.srt'),\n",
              " PosixPath('output/your_video/translations/hi/your_video_vosk_hi.srt'),\n",
              " PosixPath('output/your_video/translations/mr/your_video_vosk_mr.srt')]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if MarianMTModel is None or MarianTokenizer is None:\n",
        "    raise ImportError('transformers MarianMT not available; install transformers for translation')\n",
        "\n",
        "_translation_cache: Dict[str, Any] = {}\n",
        "\n",
        "def get_translation_model(src_lang: str, tgt_lang: str):\n",
        "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
        "    if model_name not in _translation_cache:\n",
        "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "        model = MarianMTModel.from_pretrained(model_name)\n",
        "        if torch:\n",
        "            model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        _translation_cache[model_name] = (tokenizer, model)\n",
        "    return _translation_cache[model_name]\n",
        "\n",
        "def chunk_list(items: List[Any], chunk_size: int = 8):\n",
        "    for idx in range(0, len(items), chunk_size):\n",
        "        yield items[idx: idx + chunk_size]\n",
        "\n",
        "def translate_segments(segments: List[Dict[str, Any]], src_lang: str, tgt_lang: str) -> List[Dict[str, Any]]:\n",
        "    tokenizer, model = get_translation_model(src_lang, tgt_lang)\n",
        "    device = next(model.parameters()).device if torch else 'cpu'\n",
        "    translated_segments = []\n",
        "    for batch in chunk_list(segments, chunk_size=8):\n",
        "        texts = [seg['text'] for seg in batch]\n",
        "        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "        if torch:\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs)\n",
        "        else:\n",
        "            outputs = model.generate(**inputs)\n",
        "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        for seg, text in zip(batch, decoded):\n",
        "            seg_copy = dict(seg)\n",
        "            seg_copy['text'] = text\n",
        "            translated_segments.append(seg_copy)\n",
        "    return translated_segments\n",
        "\n",
        "translated_paths = []\n",
        "for model_name, segments in transcripts_by_model.items():\n",
        "    for tgt in resolved_targets:\n",
        "        try:\n",
        "            translated = translate_segments(segments, src_lang='en', tgt_lang=tgt)\n",
        "            out_dir = ensure_dir(TRANSLATION_DIR / tgt)\n",
        "            out_path = out_dir / f'{BASE_NAME}_{model_name}_{tgt}.srt'\n",
        "            segments_to_srt(translated, out_path)\n",
        "            preview_segments(f'{model_name.upper()} -> {tgt.upper()}', translated)\n",
        "            translated_paths.append(out_path)\n",
        "            print(f'Saved -> {out_path}')\n",
        "        except Exception as exc:\n",
        "            print(f'Translation skipped for {model_name} -> {tgt}: {exc}')\n",
        "\n",
        "translated_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d21cc62c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
